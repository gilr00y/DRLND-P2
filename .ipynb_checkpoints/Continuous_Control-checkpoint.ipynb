{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# env.close()\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis20/Reacher.x86_64')\n",
    "# env = UnityEnvironment(file_name='Reacher20.app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "# print(brain.vector_)\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# print(states)\n",
    "actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "# print(actions)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     16,
     29
    ]
   },
   "outputs": [],
   "source": [
    "# Two networks\n",
    "# Actor:\n",
    "# * Takes in State, outputs probability distribution over actions\n",
    "# * Get action A to take from given state S\n",
    "# * Observe result of taking A, yielding reward R and new state S'\n",
    "# * now we have a tuple (S,A,R,S')\n",
    "# * Hand (S,A,R,S') off to critic to estimate r + \\gamma * V(s';\\theta)\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 1200       # minibatch size\n",
    "GAMMA = 0.99         # discount factor\n",
    "MAX_TAU = 5e-4              # for soft update of target parameters\n",
    "MIN_TAU = 1e-5\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 5e-4        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     5,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(\n",
    "            self.bn1(\n",
    "                self.fc1(state)))\n",
    "        x = F.relu(\n",
    "            self.bn2(\n",
    "                self.fc2(x)))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        #JODO: Why is the output only one value???\n",
    "        # Answer: It is the expected value of taking `action` given `state`\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #JODO: Why reset parameters? When do we do it?\n",
    "        # Answer: when the network is first instantiated.\n",
    "        # What is uniform_ ?\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.relu(\n",
    "            self.bn1(\n",
    "                self.fcs1(state)))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(\n",
    "            self.bn2(\n",
    "                self.fc2(x)))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     110,
     134
    ]
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.replay_buffer = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        self.tau = MAX_TAU\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            experiences = self.replay_buffer.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval() # Sets \"train\" to False\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy() # this calls Actor#forward\n",
    "        self.actor_local.train() # Sets \"train\" to True\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_future = self.actor_target(next_states)\n",
    "        Q_future = self.critic_target(next_states, actions_future)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_future * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # Why is this the actor loss??\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor_local, self.actor_target, self.tau)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "#         print(self.mu, self.theta, self.sigma, random.gauss(self.mu, self.sigma));raise\n",
    "#         dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * random.gauss(self.mu, self.sigma)/2\n",
    "#         print(dx);raise\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        for i in range(len(state)):\n",
    "            e = self.experience(state[i], action[i], reward[i], next_state[i], done[i])\n",
    "            self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=33, action_size=4, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 16.791014\tTAU=7.498358238842906e-05\tAVG=16.79\n",
      "Episode 171\tSCORE=37.23899916764368\tTAU=2.548299677913605e-05\tAVG=30.1710"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c55213862083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-c55213862083>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m                         \u001b[0;31m# get reward (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m                        \u001b[0;31m# see if episode finished               # update the score (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8f86324ba736>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Learn, if enough samples are available in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-8f86324ba736>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 11/21 18:40: 4.64 after 100\n",
    "# 11/21 19:58: 7.50 after 100 w BatchNorm on Critic\n",
    "# 11/21 20:51: 0.93 after 30 w TAU=1e-4\n",
    "# 11/21 22:15: 5.75 after 100 w TAU=1e-3, LR_ACTOR=1e-4, LR_CRITIC=1e-4, BATCH_SIZE=1024\n",
    "# 11/22 02:30: 9.55 after 1000 w TAU=1e-4\n",
    "# TODO:\n",
    "# * Check Noise implementation\n",
    "# * \n",
    "scores = []\n",
    "def ddpg(n_episodes=2000, max_t=50000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        if agent.tau > MIN_TAU:\n",
    "            agent.tau = agent.tau - 0.02 * (agent.tau - MIN_TAU)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished               # update the score (for each agent)\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += np.mean(rewards)\n",
    "            if max(dones):\n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tSCORE={}\\tTAU={}\\tAVG={:.2f}'.format(\n",
    "            i_episode, score, agent.tau, np.mean(scores_deque)), end=\"\")\n",
    "        os.write(1, '\\rEpisode {}\\tSCORE={}\\tTAU={}\\tAVG={:.2f}'.format(\n",
    "            i_episode, score, agent.tau, np.mean(scores_deque)).encode())\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "ddpg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.16921932566539"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores[-100:])/len(scores[-100:])\n",
    "# agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "# agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "# state = env.reset()\n",
    "# for t in range(200):\n",
    "#     action = agent.act(state, add_noise=False)\n",
    "#     env.render()\n",
    "#     state, reward, done, _ = env.step(action)[brain_name]\n",
    "#     if done:\n",
    "#         break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
